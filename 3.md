## 3. Decision theory

### 3.1 Decision theory
### 3.2 Minimizing conditional expected loss
- 데이터 $x, y$를 왜 random variable로 가정할까?
	- "데이터"라는 말 자체가 벌써 현실에서의 무언가를 얘기하는 것이기 때문에 우리가 찾고자 하는 정답 즉 true $y = f(x)$ 가 있다고 가정하면, 데이터 $(x_1, y_1)$ 은 $y_1 = f(x_1)$를 만족하는 것이 아니고 $y_1 = f(x_1 + \epsilon_1) + \epsilon_2$ 라고 할 수 있겠죠. 근데 error($\epsilon$) 는 (확률적인) 분포로 줄 수 밖에 없는게 error 를 deterministic 하게 주면 사실 그건 error 가 아니죠. 하나 첨가하자면, 꼭 저렇게  RV 로 설정해야만 한다는 당위적인게 아니라  RV 로 놓고 확률적 접근을 했더니 일단 문제가 잘 정의되고,어느 정도 잘 해결도 되고 괜찮은 이론도 만들 수 있더라 뭐 이런 식이죠
### 3.3 Choosing f to minimize expected loss
### 3.4 Square loss
- [Smooth function - Wikipedia](https://en.wikipedia.org/wiki/Smoothness)
- 왜 squared loss를 계산할 때 $p(y|x)$를 smooth function으로 가정할까?
	- smooth function 얘기가 나오면 보통 두 가지를 의미할 수 있는데요 (1) 무한번 미분가능 (2) 적어도 한번 미분가능하고 도함수가 연속. 여기서는 편하게 충분히 미분가능하다고 가정하네요. loss의 최소값을 구하기 위해서 미분을 하기 때문에 이를 위해서 smooth를 가정합니다. 
### 3.5 The Big picture (part 1)
- [Conditional probability - sum & product rule - stackexchange](https://math.stackexchange.com/questions/873523/conditional-probability-about-sum-and-product-rule)
### 3.6 The Big picture (part 2)
- [Exact inference - cmu lecture](https://www.cs.cmu.edu/~epxing/Class/10708-14/scribe_notes/scribe_note_lecture4.pdf)
### 3.7 The Big picture (part 3) 
- CNN 최적 parameter를 추정하는 문제에서 MLE가 어떻게 사용될까?
	- discriminative model 관점에서의 parameter가 존재함 (not distribution) MLE 방법은 discriminative model의 parameter를 maximum likelihood estimation으로 추정함. 실제 데이터상으로.. 예를 들어 binary classification이라고 생각한다면  ($x$, class($y$)) 쌍의 데이터를 이런 식으로 수식으로 썼기 때문에 MLE라는 말이 붙어도 된다고 생각합니다. 아래의 수식처럼 데이터에서 class(1또는 0)이 나타난 것을 아래처럼 x를 이용해 확률분포로 $p(y|x)^n * (1-p(y|x))^{1-n}$ 표현 할 수 있고 이 식(분포)를 최대화 해야 하기 때문에, MLE라고 볼 수도 있을 것 같습니다. 
